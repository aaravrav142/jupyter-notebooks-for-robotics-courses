{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7.1. EKF SLAM\n",
    "\n",
    "## Theoretical background\n",
    "\n",
    "Simultaneous Localization And Mapping or SLAM is one of the most fundamental problems in robotics\n",
    "It arises when our robot doesn't have access to a map of the environment nor the pose where it is located.\n",
    "It is a more difficult problem than the two separate problems we did in the previous units.\n",
    "\n",
    "There are two variants of the SLAM problem:\n",
    "\n",
    "- **Online SLAM** (the one we will implement): Which only estimates the latest pose. $p(x_{k}, m_{1:L} | z_{1:k}, u_{1:k})$\n",
    "- **Full SLAM**: Estimates the whole path traversed at each step. $p(x_{1:k}, m_{1:L} | z_{1:k}, u_{1:k})$\n",
    "\n",
    "The EKF algorithm was originally one of the most influential approach to the SLAM problem.\n",
    "\n",
    "This approach is similar to the one we took to the problems of Localization and Mapping.\n",
    "\n",
    "\n",
    "\n",
    "The exercise’s appendix includes a code implementing a SLAM algorithm based on the\n",
    "Extended Kalman Filter, but it’s incomplete at some points.\n",
    "\n",
    "**Example**\n",
    "\n",
    "The following image is an exaple of a correct execution of the demo:\n",
    "\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "  <img src=\"images/fig7-1-1.png\" width=\"400\" alt=\"\">\n",
    "  <figcaption>\n",
    "      Fig. 1: Execution of the EKF algorithmn for SLAM. <br/>\n",
    "      it shows 3 poses: true (blue), expected (red) and estimated (green + confidence ellipse); <br/>\n",
    "      true landmarks (big multicolored squares), and \n",
    "      their final <br/> estimations (green squares + red confidence ellipse)\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from scipy import linalg\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from utils.tcomp import tcomp\n",
    "from utils.Jacobians import J1, J2\n",
    "from utils.DrawRobot import DrawRobot\n",
    "from utils.PlotEllipse import PlotEllipse\n",
    "from utils.AngleWrap import AngleWrap\n",
    "from utils.unit7.FOV import FOVSensor\n",
    "from utils.unit7.Jacobians import GetNewFeatureJacs, GetObsJacs\n",
    "from utils.unit7.MapCanvas import MapCanvas\n",
    "from utils.unit7.Robot import EFKSlamRobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify a bit the code we will use the following classes:\n",
    "\n",
    "- [FOVSensor](/edit/utils/unit7/FOV.py): Take a look at the parameters it contains and its functions.\n",
    "- [EFKSlamRobot](/edit/utils/unit7/Robot.py): We'll only use its parameters and step function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the missing parts of the code and then answer the following questions:\n",
    "\n",
    "- 1. What represents PPred and why is it build in that way in the EFK function? Which are its dimensions? and those of the matrices used to build it? (`PPredvv`, `PPredvm` and `PPredmm`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2. Build the state Jacobian jH used in the Kalman filter during the update step when a previously perceived landmark is seen again. Employ the output of the `GetObsJacs` function. Analyze both its size and its content throughout the SLAM simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EFKSlam(robot: EFKSlamRobot, sensor: FOVSensor, z, iFeature, u):\n",
    "    \"\"\" Implementation of the EFK algorithm for SLAM.\n",
    "    \n",
    "        It does not return anything.\n",
    "        Just updates the state attributes in robot(causing side effects only in robot). \n",
    "    \n",
    "        Args:\n",
    "            robot\n",
    "            sensor\n",
    "            z: observation made in this loop\n",
    "            iFeature: Index of the landmark observed in the world map and in robot.MappedFeatures.\n",
    "                It serves to chech whether it is in the state and if so, where is located.\n",
    "            u: Movement command received in this loop. \n",
    "                It serves us to predict the future pose in the state(xVehicle).\n",
    "                At the time this function is called, robot.pose and robot.true_pose have been updated already.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Useful vbles\n",
    "    xVehicle = robot.xEst[0:3]\n",
    "    xMap = robot.xEst[3:]\n",
    "\n",
    "    #\n",
    "    # Prediction step\n",
    "    #\n",
    "    xVehiclePred = tcomp(xVehicle,u)\n",
    "\n",
    "    j1 = J1(xVehicle,u)\n",
    "    j2 = J2(xVehicle,u)\n",
    "\n",
    "    PPredvv = j1 @ robot.PEst[0:3,0:3] @ j1.T + j2@ robot.cov_move @ j2.T\n",
    "    PPredvm = j1@robot.PEst[0:3,3:]\n",
    "    PPredmm = robot.PEst[3:,3:]\n",
    "\n",
    "    xPred = np.vstack([xVehiclePred,xMap])\n",
    "    PPred = np.vstack([\n",
    "        np.hstack([PPredvv, PPredvm]),\n",
    "        np.hstack([PPredvm.T, PPredmm])\n",
    "    ])\n",
    "\n",
    "    #\n",
    "    # Update step\n",
    "    #\n",
    "    if z.shape[1] > 0:\n",
    "        #have we seen this feature before?\n",
    "        if robot.MappedFeatures[iFeature,0] >=0:\n",
    "\n",
    "            #predict observation: find out where it is in state vector\n",
    "            FeatureIndex = robot.MappedFeatures[iFeature,:]\n",
    "            xFeature = xPred[FeatureIndex[0]:FeatureIndex[1]]\n",
    "\n",
    "            zPred = sensor.observe(xVehiclePred,xFeature, noisy=False)\n",
    "\n",
    "            # get observation Jacobians\n",
    "            jHxv,jHxf = GetObsJacs(xVehicle,xFeature)\n",
    "            # Fill in state jacobian\n",
    "\n",
    "            #\n",
    "            # TODO Point 2, Build jH from JHxv and jHxf\n",
    "            #\n",
    "            \n",
    "            raise NotImplementedError\n",
    "            jH = None\n",
    "\n",
    "            # Do Kalman update:\n",
    "            Innov = z-zPred\n",
    "            Innov[1] = AngleWrap(Innov[1])\n",
    "\n",
    "            S = jH@PPred@jH.T + sensor.cov_sensor\n",
    "            W = PPred@jH.T@linalg.inv(S)\n",
    "            \n",
    "            robot.xEst = xPred+ W@Innov\n",
    "            robot.PEst = PPred-W@S@W.T\n",
    "\n",
    "            #ensure P remains symmetric\n",
    "            robot.PEst = 0.5*(robot.PEst+robot.PEst.T)\n",
    "        else:\n",
    "            # this is a new feature add it to the map....\n",
    "            nStates = len(robot.xEst)\n",
    "\n",
    "            xFeature = (\n",
    "                xVehiclePred[0:2] + \n",
    "                np.vstack([\n",
    "                    z[0]*np.cos(z[1]+xVehiclePred[2]),\n",
    "                    z[0]*np.sin(z[1]+xVehiclePred[2])\n",
    "                ])\n",
    "            )\n",
    "            robot.xEst = np.vstack([xPred,xFeature]) #augmenting state vector\n",
    "            jGxv, jGz = GetNewFeatureJacs(xVehicle,z)\n",
    "            \n",
    "            M = np.vstack([\n",
    "                np.hstack([np.eye(nStates), np.zeros((nStates,2))]),# note we don't use jacobian w.r.t vehicle\n",
    "                np.hstack([jGxv, np.zeros((2,nStates-3)), jGz]),\n",
    "            ])\n",
    "            robot.PEst = M@linalg.block_diag(robot.PEst,sensor.cov_sensor)@M.T\n",
    "\n",
    "            #remember this feature as being mapped we store its ID and position in the state vector\n",
    "            robot.MappedFeatures[iFeature,:] = [len(robot.xEst)-2, len(robot.xEst)]\n",
    "    else:\n",
    "        robot.xEst = xPred\n",
    "        robot.PEst = PPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following demo to test our robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(robot,\n",
    "         sensor,\n",
    "         nFeatures=10,\n",
    "         MapSize=200,\n",
    "         DrawEveryNFrames=5,\n",
    "         nSteps = 195,\n",
    "         turning = 50,\n",
    "         mode='one_landmark_in_fov',\n",
    "         NONSTOP=True,\n",
    "         LOG=False):\n",
    "    \n",
    "    \n",
    "    %matplotlib notebook\n",
    "    if not NONSTOP:\n",
    "        matplotlib.use('TkAgg')\n",
    "\n",
    "\n",
    "    seed = 100\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    logger = Logger(nFeatures, nSteps);\n",
    "    \n",
    "    # Map configuration\n",
    "    Map = MapSize*random.rand(2, nFeatures) - MapSize/2\n",
    "\n",
    "    # Matplotlib setup\n",
    "    canvas = MapCanvas(Map, MapSize, nFeatures, robot, sensor, NONSTOP)\n",
    "    canvas.initialFrame(robot, Map, sensor)\n",
    "\n",
    "    u = np.vstack([3.0, 0.0, 0.0])\n",
    "\n",
    "    for k in range(1, nSteps):\n",
    "        \n",
    "        # Move the robot with a control action u\n",
    "        u[2] = 0.0\n",
    "        if k%turning == 0:\n",
    "             u[2]=np.pi/2\n",
    "\n",
    "        robot.step(u)\n",
    "\n",
    "        # Get new observation/s\n",
    "        if mode == 'one_landmark_in_fov' :\n",
    "            # Get a random observations within the fov of the sensor\n",
    "            z, iFeature = sensor.random_observation(robot.true_pose, Map, fov=True)\n",
    "        elif mode == 'landmarks_in_fov':\n",
    "            # OPTIONAL Point 4\n",
    "            raise NotImplementedError\n",
    "\n",
    "        EFKSlam(robot, sensor, z, iFeature, u)\n",
    "\n",
    "        # Point 3, Robot pose and features localization errors and determinants\n",
    "        if logger is not None:\n",
    "            logger.log(k, robot, Map) \n",
    "\n",
    "        # Drawings\n",
    "        if k % DrawEveryNFrames == 0:\n",
    "            canvas.drawFrame(robot, sensor, Map, iFeature)\n",
    "            \n",
    "    # Draw the final estimated positions and uncertainties of the features\n",
    "    canvas.drawFinal(robot)\n",
    "\n",
    "    if logger is not None:\n",
    "        %matplotlib inline\n",
    "        logger.draw(canvas.colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3. Store in each iteration the determinant of the matrices of covariance of the robot pose and the localization of each feature and plot them. Do the same with the error of the localization of the pose and the features. Use the variables `PFeatDetStore`, `FeatErrStore`, `PXErrStore` and `XErrStore`.\n",
    "\n",
    "  Once this has been completed, the results will look similar to the following figutes:\n",
    "  \n",
    "  <table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/fig7-1-2.png\" width=\"300\"></td>\n",
    "        <td><img src=\"images/fig7-1-3.png\" width=\"300\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/fig7-1-4.png\" width=\"300\"></td>\n",
    "        <td><img src=\"images/fig7-1-5.png\" width=\"300\"></td>\n",
    "    <tr>\n",
    "  </table>\n",
    "  \n",
    "  Just as in the previous practice you'll have to complete a Logger class that is used along the demo in `main()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "    def __init__(self, nFeatures, nSteps):\n",
    "        # Storage:\n",
    "        self.PFeatDetStore = np.full((nFeatures,nSteps),np.Inf)\n",
    "        self.FeatErrStore = np.full((nFeatures,nSteps),np.Inf)\n",
    "        self.PXErrStore = np.full((nSteps,1), 0)\n",
    "        self.XErrStore = np.full((2,nSteps), 0) # error in position and angle\n",
    "        \n",
    "    def log(self, k, robot, Map):\n",
    "        \n",
    "        # TODO. Store relevant info\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def draw(self, colors):\n",
    "        nSteps = self.PFeatDetStore.shape[1]\n",
    "        nFeatures = self.PFeatDetStore.shape[0]\n",
    "        \n",
    "        plt.figure(2) #hold on\n",
    "        plt.title('Errors in robot localization')\n",
    "        plt.plot(self.XErrStore[0,:],'b',label=\"Error in position\")\n",
    "        plt.plot(self.XErrStore[1,:],'r',label=\"Error in orientation\")\n",
    "        #plt.legend('Error in position','Error in orientation')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.figure(3)# hold on\n",
    "        plt.title('Determinant of the cov. matrix associated to the robot localization')\n",
    "        xs = np.arange(nSteps)\n",
    "        plt.plot(self.PXErrStore[:],'b')\n",
    "\n",
    "        plt.figure(4)# hold on\n",
    "        plt.title('Errors in features localization')\n",
    "\n",
    "        plt.figure(5)# hold on\n",
    "        plt.title('Log of the determinant of the cov. matrix associated to each feature')\n",
    "\n",
    "        for i in range(nFeatures):\n",
    "            plt.figure(5)\n",
    "            h = plt.plot(np.log(self.PFeatDetStore[i,:]), color=colors[i,:])\n",
    "            plt.figure(4)\n",
    "            h = plt.plot(self.FeatErrStore[i,:], color=colors[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - 3.1 - Now the program is complete. Run it a few times and describe the obtained results. Explain the meaning of each element appearing in the figure resulting from the execution of the SLAM algorithm. (See a running example in the next page).\n",
    "    - 3.2 - Play with different numbers of landmarks and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map configuration\n",
    "n_features = 10\n",
    "MapSize = 200\n",
    "\n",
    "# Robot base characterization\n",
    "SigmaX = 0.01 # Standard deviation in the x axis\n",
    "SigmaY = 0.01 # Standard deviation in the y axins\n",
    "SigmaTheta = 1.5*np.pi/180 # Bearing standar deviation\n",
    "R = np.diag([SigmaX**2, SigmaY**2, SigmaTheta**2]) # Cov matrix\n",
    "\n",
    "xRobot = np.vstack([-MapSize/3, -MapSize/3, 0.0])\n",
    "robot = EFKSlamRobot(xRobot, R, n_features)\n",
    "\n",
    "Sigma_r = 1.1\n",
    "Sigma_theta = 5*np.pi/180\n",
    "Q = np.diag([Sigma_r, Sigma_theta])**2 # Covariances for our very bad&expensive sensor (in the system <d,theta>)\n",
    "fov = np.pi*2/3\n",
    "max_range = 100\n",
    "\n",
    "sensor = FOVSensor(Q, fov, max_range) \n",
    "\n",
    "#main(robot, sensor, nFeatures=n_features, MapSize=MapSize, NONSTOP=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4. Optional. The provided code only employs a feature per iteration. Change the code to consider all the features within the FOV of the robot during the update step of the EKF. Creating a backup is recommended before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
